{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86PFykvan0Ah"
      },
      "source": [
        "Creazione del dataset italiano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnLt6llOp-Fj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SySBf_xZnvgH"
      },
      "outputs": [],
      "source": [
        "pip install recipe-scrapers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk==3.8.1"
      ],
      "metadata": {
        "id": "7NjR3ND3PQE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas===1.5.2\n"
      ],
      "metadata": {
        "id": "VvFlASURSy1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4==4.11.1"
      ],
      "metadata": {
        "id": "783aE8RvTI5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "UjLplb03P62w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAUSE_TIME = 5\n",
        "PATH = '/content/drive/MyDrive/tesi/dataset/'"
      ],
      "metadata": {
        "id": "-p633tU-2VEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import della lista di ingredienti da Giallo Zafferano"
      ],
      "metadata": {
        "id": "jqmnjSzDD_WY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-W_UiGjowv6"
      },
      "outputs": [],
      "source": [
        "#giallo zafferano ingredienti\n",
        "from recipe_scrapers import scrape_me\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL_RICETTE = 'https://www.giallozafferano.it/ricette-cat/page'\n",
        "END_SCRAPING = False\n",
        "MAX_LINES = 500\n",
        "f = open(PATH + 'ingredients_giallo.json', 'w')\n",
        "count = 1\n",
        "while(not END_SCRAPING):\n",
        "  time.sleep(PAUSE_TIME)\n",
        "  page_ricetta = requests.get(URL_RICETTE + str(count))\n",
        "  soup = BeautifulSoup(page_ricetta.text, 'html.parser') \n",
        "  ricette = iter(soup.findAll('div', class_='gz-link-more-recipe'))\n",
        "  while (ricetta_item := next(ricette, None)) is not None and not END_SCRAPING:\n",
        "    ricetta = scrape_me(ricetta_item.find('a').get('href'))\n",
        "    ingredienti = iter(ricetta.ingredients())\n",
        "    while (ingrediente := next(ingredienti, None)) is not None and not END_SCRAPING:\n",
        "      if(count<=MAX_LINES):\n",
        "        f.write(json.dumps({\n",
        "          'text': ingrediente\n",
        "        }))\n",
        "        f.write('\\n')\n",
        "        #print(\"Ricetta \"+ str(count)+\" di \"+ str(MAX_LINES))\n",
        "        count += 1 \n",
        "      else:\n",
        "        END_SCRAPING = True\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import della lista di ingredienti da Mysia.info"
      ],
      "metadata": {
        "id": "Zm3v1ALHEHTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mysia.info ingredienti\n",
        "from recipe_scrapers import scrape_me\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL_RICETTE = 'https://www.misya.info/ricette/page/'\n",
        "END_SCRAPING = False\n",
        "MAX_LINES = 500\n",
        "f = open(PATH + 'ingredients_mysia.json', 'w')\n",
        "count = 1\n",
        "while(not END_SCRAPING):\n",
        "  page_ricetta = requests.get(URL_RICETTE + str(count))\n",
        "  soup = BeautifulSoup(page_ricetta.text, 'html.parser') \n",
        "  ricette = iter(soup.findAll('div', class_='ricetta'))\n",
        "  while (ricetta_item := next(ricette, None)) is not None and not END_SCRAPING:\n",
        "    time.sleep(PAUSE_TIME)\n",
        "    ricetta = scrape_me(ricetta_item.find('a', class_ = 'cont-foto').get('href'))\n",
        "    ingredienti = iter(ricetta.ingredients())\n",
        "    while (ingrediente := next(ingredienti, None)) is not None and not END_SCRAPING:\n",
        "      if(count<=MAX_LINES):\n",
        "        f.write(json.dumps({\n",
        "          'text': ingrediente\n",
        "        }))\n",
        "        f.write('\\n')\n",
        "        #print(\"Ricetta \"+ str(count)+\" di \"+ str(MAX_LINES))\n",
        "        count += 1 \n",
        "      else:\n",
        "        END_SCRAPING = True\n",
        "f.close()"
      ],
      "metadata": {
        "id": "TFvC0raI_slZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import della lista di istruzioni da Giallo Zafferano"
      ],
      "metadata": {
        "id": "0-GRZX2nEO84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#giallo zafferano istruzioni\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "from recipe_scrapers import scrape_me\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "URL_RICETTE = 'https://www.giallozafferano.it/ricette-cat/page'\n",
        "END_SCRAPING = False\n",
        "MAX_LINES = 100\n",
        "f = open(PATH + 'instructions_giallo.json', 'w')\n",
        "count = 1\n",
        "while(not END_SCRAPING):\n",
        "  page_ricetta = requests.get(URL_RICETTE + str(count))\n",
        "  soup = BeautifulSoup(page_ricetta.text, 'html.parser') \n",
        "  ricette = iter(soup.findAll('div', class_='gz-link-more-recipe'))\n",
        "  while (ricetta_item := next(ricette, None)) is not None and not END_SCRAPING:\n",
        "    time.sleep(PAUSE_TIME)\n",
        "    ricetta = scrape_me(ricetta_item.find('a').get('href'))\n",
        "    istruzioni_list = iter(ricetta.instructions_list())\n",
        "    while (istruzione_step := next(istruzioni_list, None)) is not None and not END_SCRAPING:\n",
        "      istruzioni = iter(sent_tokenize(istruzione_step))\n",
        "      while (istruzione := next(istruzioni, None)) is not None and not END_SCRAPING:\n",
        "        if(count<=MAX_LINES):\n",
        "          f.write(json.dumps({\n",
        "            'text': istruzione\n",
        "          }))\n",
        "          f.write('\\n')\n",
        "          #print(\"Ricetta \"+ str(count)+\" di \"+ str(MAX_LINES))\n",
        "          count += 1 \n",
        "        else:\n",
        "          END_SCRAPING = True\n",
        "f.close()"
      ],
      "metadata": {
        "id": "APH_JQ4MDTZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import della lista di istruzioni da Mysia.info"
      ],
      "metadata": {
        "id": "_Wz7K-UbEK5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mysia.info istruzioni\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from recipe_scrapers import scrape_me\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import sent_tokenize\n",
        "URL_RICETTE = 'https://www.misya.info/ricette/page/'\n",
        "END_SCRAPING = False\n",
        "MAX_LINES = 100\n",
        "f = open(PATH + 'instructions_mysia.json', 'w')\n",
        "count = 1\n",
        "while(not END_SCRAPING):\n",
        "  page_ricetta = requests.get(URL_RICETTE + str(count))\n",
        "  soup = BeautifulSoup(page_ricetta.text, 'html.parser') \n",
        "  ricette = iter(soup.findAll('div', class_=\"ricetta\"))\n",
        "  while (ricetta_item := next(ricette, None)) is not None and not END_SCRAPING:\n",
        "    time.sleep(PAUSE_TIME)\n",
        "    ricetta = scrape_me(ricetta_item.find(\"a\", class_ = \"cont-foto\").get('href'))\n",
        "    istruzioni_list = iter(ricetta.instructions_list())\n",
        "    while (istruzione_step := next(istruzioni_list, None)) is not None and not END_SCRAPING:\n",
        "      istruzioni = iter(sent_tokenize(istruzione_step))\n",
        "      while (istruzione := next(istruzioni, None)) is not None and not END_SCRAPING:\n",
        "        if(count<=MAX_LINES):\n",
        "          f.write(json.dumps({\n",
        "            'text': istruzione\n",
        "          }))\n",
        "          f.write('\\n')\n",
        "          #print(\"Ricetta \"+ str(count)+\" di \"+ str(MAX_LINES))\n",
        "          count += 1 \n",
        "        else:\n",
        "          END_SCRAPING = True\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8fo0eSZaCK_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Titolo ricette Giallo"
      ],
      "metadata": {
        "id": "bLiGXfRWSgCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#giallo zafferano titolo\n",
        "from recipe_scrapers import scrape_me\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL_RICETTE = 'https://www.giallozafferano.it/ricette-cat/page'\n",
        "END_SCRAPING = False\n",
        "MAX_LINES = 100\n",
        "f = open(PATH + 'title_giallo.json', 'w')\n",
        "count = 1\n",
        "while(not END_SCRAPING):\n",
        "  time.sleep(PAUSE_TIME)\n",
        "  page_ricetta = requests.get(URL_RICETTE + str(count))\n",
        "  soup = BeautifulSoup(page_ricetta.text, 'html.parser') \n",
        "  ricette = iter(soup.findAll('div', class_='gz-link-more-recipe'))\n",
        "  while (ricetta_item := next(ricette, None)) is not None and not END_SCRAPING:\n",
        "    ricetta = scrape_me(ricetta_item.find('a').get('href'))\n",
        "    if(count<=MAX_LINES):\n",
        "      f.write(json.dumps({\n",
        "          'text': ricetta.title()\n",
        "      }))\n",
        "      f.write('\\n')\n",
        "      #print(\"Ricetta \"+ str(count)+\" di \"+ str(MAX_LINES))\n",
        "      count += 1 \n",
        "    else:\n",
        "      END_SCRAPING = True\n",
        "f.close()"
      ],
      "metadata": {
        "id": "NzZ2pGpkQnYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import di frasi dalle presentazioni di ricette di Giallo Zafferano"
      ],
      "metadata": {
        "id": "85teO62YHP-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisi USDA"
      ],
      "metadata": {
        "id": "pAIDVEDBvDTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/tesi/usda/' +'food.csv')['description'].str.split(',', expand=True)\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "i = 0\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/tesi/usda/output.xlsx') as writer: \n",
        "  for column in df:\n",
        "    df_col = pd.DataFrame(df[df.columns[i]].sort_values(ascending=True).unique())\n",
        "    df_col.to_excel(writer,\n",
        "             sheet_name='column'+str(i))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "JEkwvGO0vGKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from dataclasses import make_dataclass\n",
        "Adjective = make_dataclass(\"Adjective\", [(\"adj\", str), (\"sentence\", str)])\n",
        "df = pd.read_csv('/content/drive/MyDrive/tesi/usda/' +'food.csv')['description'].str.split(',', expand=True)\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/tesi/usda/adjectives_and_row.xlsx') as writer: \n",
        "  for column in df:\n",
        "    df_col = pd.DataFrame(df[df.columns[column]].sort_values(ascending=True).unique())\n",
        "    df_adj = pd.DataFrame()\n",
        "    for index, row in df_col.iterrows():\n",
        "      if(row.values[0]!=None):\n",
        "        tokens = nltk.word_tokenize(row.values[0])\n",
        "        tags = nltk.pos_tag(tokens)\n",
        "        for word, pos in tags:\n",
        "          if pos.startswith(\"JJ\"):\n",
        "            df_adj = df_adj.append([Adjective(word, row.values[0])], ignore_index=True)\n",
        "            print(f\"{word} is an adjective.\")\n",
        "          else:\n",
        "            print(f\"{word} is not an adjective.\")\n",
        "   # if(df_adj.size>0):\n",
        "    #  df_adj = pd.DataFrame(df_adj.sort_values(by='0',ascending=True).unique())\n",
        "    df_adj.to_excel(writer,\n",
        "             sheet_name='column'+str(column))"
      ],
      "metadata": {
        "id": "EfpBa45zpXK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U deep-translator"
      ],
      "metadata": {
        "id": "91w_7_RM29SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import time\n",
        "import pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "from dataclasses import make_dataclass\n",
        "Adjective = make_dataclass(\"Adjective\", [(\"adj\", str), (\"sentence\", str)])\n",
        "df = pd.read_csv('/content/drive/MyDrive/tesi/usda/' +'food.csv')['description'].str.split(',', expand=True)\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/tesi/usda/adjectivesita.xlsx') as writer: \n",
        "  for column in df:\n",
        "    df_col = pd.DataFrame(df[df.columns[column]].sort_values(ascending=True).unique())\n",
        "    df_adj = pd.DataFrame()\n",
        "    for index, row in df_col.iterrows():\n",
        "      if(row.values[0]!=None):\n",
        "        tokens = nltk.word_tokenize(row.values[0])\n",
        "        tags = nltk.pos_tag(tokens)\n",
        "        for word, pos in tags:\n",
        "          if pos.startswith(\"JJ\"):\n",
        "            translated = GoogleTranslator(source='en', target='it').translate(word)\n",
        "            df_adj = df_adj.append([Adjective(translated, row.values[0])], ignore_index=True)\n",
        "            print(f\"{translated} is an adjective.\")\n",
        "            time.sleep(1)\n",
        "          else:\n",
        "            print(f\"{word} is not an adjective.\")\n",
        "    #if(df_adj.size>0):\n",
        "     # df_adj = pd.DataFrame(df_adj.sort_values(by='0',ascending=True).unique())\n",
        "    df_adj.to_excel(writer,\n",
        "             sheet_name='column'+str(column))"
      ],
      "metadata": {
        "id": "vYd2usLJ2wsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisi del numero di occorrenze"
      ],
      "metadata": {
        "id": "gGVSFhNodJ-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from dataclasses import make_dataclass\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/tesi/usda/' +'food.csv')['description'].str.split(',', expand=True)\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "i = 0\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/tesi/usda/output.xlsx') as writer: \n",
        "  for column in df:\n",
        "    df_col = pd.DataFrame(df.groupby(df.columns[i]).size())\n",
        "    df_col.to_excel(writer,\n",
        "             sheet_name='column'+str(i), header=False)\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "r1l-Ax2BdIDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}